\section{Badane algorytmy}
\label{sec:algorytmy}
W~ramach podstawowej częśći projektu, zdecydowaliśmy się na zbadanie trzech typów algorytmów grupowania bazujących na niepodobieńtswie przykładów: $k$-medoidów, hierarchicznego oraz rozmytego.

\subsection{Algorytm k-medoidów}
\label{subsec:kmeans}
Algorytm $k$-średnich jest jednym z~najbardziej podstawowych mechanizmów grupowania. Jego jest pogrupowanie danych na $k$ grup w~taki sposób, aby zminimalizować sumę kwadratów odległości euklidesowych punktów od wyznaczonych centroidów (środków) grup. Wyznaczone centroidy są średnią wszystkich punktów należących do jego grupy, co oznacza że centroid nie jest ściśle związany z~żadnym przykładem oraz że algorytm może być wrażliwy na wartości odstające oraz anomalie~\cite{zum}.

Z~tego powodu, w~projekcie rozważony zostanie algorytm $k$-medoidów, który przyporządkowuje centroidom jeden z~grupowanych przykładów. Co więcej, w~odróżnieniu od pierwotnej wersji, algorytm ten gwarantuje zbieżność niezależnie od zastosowanej miary niepodobieństwa.

W~pierwszym kroku algorytmu należy dobrać liczbę poszukiwanych grup oraz przyporządkować w~sposób losowy wszystkie obserwacje do grup. Następnie, w~$i$-tym kroku należy obliczyć położenie centroidu dla każdej grupy i~dla każdego punktu zaktualizować przynależności do grupy. Algorytm kończy się gdy w~danym kroku nie została zmieniona żadna przynależność lub przekroczono maksymalną liczbę kroków.

Domyślnie algorytm $k$-medoidów jest wywoływany za pomocą polecenia \texttt{pam}. Polecenie to znajduje sie~w pakiecie \texttt{cluster}.

Głównym parametrem algorytmu $k$-średnich jest liczba szukanych grup $k$. Liczbę tą podajemy $a priori$, przed uruchomieniem algorytmu. Jednym ze sposobów na odpowiedni dobór parametru $k$ jest przeszukiwanie zupełne w~rozsądnie dobranym przedziale i~znalezienie takiej wartości, dla której badany wskaźnik jakości uzyskuje najlepszą wartość.

Największym problemem algorytmu $k$-medoidów jest dobór początkowego podziału na grupy. Możliwymi sposobami są losowy wybór $k$ przykładów ze zbioru danych lub ręczne podanie zbioru centroidów początkowych.

\subsection{Algorytmy grupowania hierarchicznego}
\label{subsec:hierarchical}
Grupowanie hierarchiczne w eksploracji danych i statystyce jest metodą analizy, która ma na celu zbudowanie hierarchii klastrów. W przeciwieństwie do wielu algorytmów dzielących zbiory danych na klastry w tym wypadku nie jest konieczne wstępne określenie liczby tworzonych klastrów. Algorytmy grupowania  hierarchicznego dzielimy na dwa typy: aglomeracyjne (łączące) i deglomeracyjne (dzielące) \cite{maimonrokach}. Wynikiem użycia metod hierarchicznego grupowania jest zestaw zagnieżdżonych klastrów, które są zwykle prezentowane na~dendrogramie. Dendrogram jest to wielopoziomowa hierarchia, w której klastry z jednego poziomu są połączone i tworzą większe klastry na kolejnych poziomach.

Przy grupowaniu hierarchicznym wykorzystywane są różne sposoby na określanie połączenia między dwoma klastrami. Są to miary odległości pomiędzy dwoma klastrami. Do najbardziej popularnych, możliwych do wyboru w algorytmach wbudowanych w język R należą\cite{rodriguez2015}:

\begin{itemize}
    \item \textbf{pojedyncze połączenie} - minimalna odległość między obserwacją w jednym klastrze a obserwacją w innym klastrze,
    \item \textbf{kompletne połączenie} - maksymalna odległość między obserwacją w jednym klastrze a obserwacją w innym klastrze,
    \item \textbf{średnie połączenie} - średnia odległość między obserwacją w jednym klastrze a obserwacją w innym klastrze,
    \item \textbf{połączenie centroidalne} - odległość pomiędzy centroidami klastrów,
    \item \textbf{połączenie medianowe} - mediana odległości między obserwacją w jednym klastrze a obserwacją w innym klastrze
\end{itemize}


Należy pamiętać o tym, że w zależności od zbioru danych, niektóre metody mogą działać lepiej od innych. W projekcie podczas eksperymentów badane były połączenia: kompletne i średnie. 

\subsubsection{Algorytmy aglomeracyjne}
Metody aglomeracyjne są najbardziej popularną metodą grupowania hierarchicznego. Zaraz po rozpoczęciu algorytmu każdy obiekt ze zbioru danych jest traktowany jako pojedynczy klaster. Kolejno klastry są  łączone, do momentu gdy wszystkie klastry zostaną scalone w jeden duży klaster zawierający wszystkie obiekty zbioru danych. W języku R dokonujemy grupowania algorytmem aglomeracyjnym przy użyciu polecenia \texttt{agnes}.

\subsubsection{Algorytmy deglomeracyjne}
Metody deglomeracyjne są przeciwieństwem metod aglomeracyjnych. Na początku działania algorytmu wszystkie obserwacje znajdują się w jednym klastrze. W kolejnych iteracjach klastry są dzielone na mniejsze \cite{kassambara}. Proces dzielenia klastrów powtarzany jest do momentu, gdy każda obserwacja znajduje się we właściwym klastrze tj. do momentu gdy liczba klastrów będzie równa liczbie obserwacji. W języku R dokonujemy grupowania deglomeracyjnego przy użyciu polecenia \texttt{diana}.

\subsection{Algorytmy grupowania rozmytego}
\label{subsec:fuzzy}
Algorytmy grupowania rozmytego bazują na założeniu że pojedyncza obserwacja może równocześnie należeć do wielu grup. To jak bardzo przykład należy do danej grupy $v$, opisuje \emph{funkcja przynależności} $\mu_{v}(x)$, która może przyjmować wartości z~przedziału $[0,1]$. Dla pojedynczej obserwacji $x$ spełnione są następujące własności:

\begin{equation}
    \sum_{v=1}^{k} \mu_{v}(x)    
\end{equation}

\begin{equation}
    \forall{v} \quad \mu_{v}(x) \geq 0     
\end{equation}

Podejście rozmyte może potencjalnie dać więcej korzyści, biorąc pod uwagę temat projektu. W~nowoczesnym futbolu, co raz częściej możemy spotkać się z~sytuacją w~której zawodnicy muszą umieć grać na kilku (jak nie wszystkich) pozycjach równocześnie.

W~projekcie porównamy działanie dwóch algorytmów rozmytego grupowania: fuzzy C-means oraz FANNY.

Podstawowym algorytmem grupowania rozmytego jest rozszerzenie algorytmu $k$-średnich na zbiory rozmyte, znany w~literaturze jako rozmyte $k$-średnich lub fuzzy C-means~\cite{cmeans}.

Algorytm ten nie różni się wiele od wersji oryginalnej. Jedyną różnicą jest zmiana sposobu określania przynależności do grupy, zamiast szukania najbliższej grupy, obliczamy przynależność do wszystkich grup równocześnie. W~pierwszym kroku algorytmu należy dobrać liczbę poszukiwanych grup oraz przyporządkować w~sposób losowy początkowe wartości funkcji przynależności do wszystkich grup. Następnie, w~$i$-tym kroku należy obliczyć położenie centroidu dla każdej grupy, a~dalej dla każdego punktu zaktualizować funkcje przynależności do wszystkich grup. Algorytm kończy działanie w~momencie gdy wartości funkcji przynależności zmieniły się o~wartość mniejszą niż $\epsilon$ lub osiągnięta została maksymalna liczba kroków.

Algorytm rozmytych $k$-średnich został zaimplementowany w~pakiecie \texttt{cluster} jako funkcja \texttt{fcm}. Parametrami wymagającymi strojenia jest liczba grup, zadana miara niepodobieństwa, maksymalna liczba iteracji oraz stopień rozmycia~\cite{cmeans}.

Drugim badanym przez nas algorytmem będzie algorymt FANNY opracowany przez Kaufmana i~Rousseeuwa~\cite{kaufman2009finding} a~rozszerzony przez Martina Maechlera. Algorytm ten minimalizuje następującą funkcję celu:

\begin{equation}
    \sum_{v=1}^{k} \frac{\sum_{i=1}^{n}\sum_{j=1}^{n} \mu^{r}_{v}(x_{i}) \mu^{r}_{v}(x_{j}) d(x_{i}, x_{j})}{2 \sum_{j=1}^{n} \mu^{r}_{v}(x_{j})} 
\end{equation}

gdzie~$n$ oznacza liczbę obserwacji w~zbiorze danych, $k$~jest liczbą grup, $r$~jest \emph{wykładnikiem przynależności}, natomiast~$d(x_{i}, x_{j})$ jest wybraną funkcją niepodobieństwa przykładów~$x_{i}$~i~$x_{j}$.

Algorytm ten został zaimplementowany w~pakiecie \texttt{cluster} jako funkcja \texttt{fanny}. Parametrami tej funkcji są: liczba grup, wykładnik przynależności, funkcja odległości oraz maksymalna liczba iteracji.