\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

\usepackage{../ZUMreport}
\usepackage{times}
\usepackage{url}
\usepackage{xcolor}
\usepackage{polski}
\usepackage[polish]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[utf8]{luainputenc}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{caption}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{subfig}
\usepackage{pgfplots}
\usepackage{paracol}
\usepackage{gensymb}

\urlstyle{same}
	
\title{Zaawansowane uczenie maszynowe\\ Założenia wstępne projektu}

\author{
Robert Wojtaś, Jakub Sikora
\affiliations
numery albumów: 283234, 283418 \\
\emails
robert.wojtas.stud@pw.edu.pl, jakub.sikora2.stud@pw.edu.pl
}

\newcommand{\todo}[1]{\textcolor{red}{\textbf{TO DO:} #1}}
\newcommand{\jsc}[1]{\textcolor{blue}{\textbf{JS:} #1}}
\newcommand{\rwc}[1]{\textcolor{green}{\textbf{RW:} #1}}

\begin{document}
\maketitle

\noindent \textbf{Temat projektu}: Grupowanie (G), \\
\textbf{Zestaw danych}: Dane FIFA 2019 Complete Player Dataset (Kaggle) -- grupowanie lub predykcja skuteczności zawodników.

\section{Cel projektu}
\label{sec:cel-projektu}
Celem projektu będzie zbadanie użyteczności algorytmów grupowania w~zadaniu dobierania optymalnej pozycji piłkarzy z~gry FIFA~19 na boisku. Umiejętności poszczególnych zawodników są opisane za pomocą szeregu parametrów, które mają za zadanie obrazować zdolności gry w~ataku, pomocy i~obronie. Zadaniem algorytmu grupowania będzie połączenie w~grupy zawodników o~podobnych umiejętnościach, tak aby na tej podstawie móc wnioskować o~ich optymalnej pozycji.

\section{Zbiór danych}
\label{sec:dane}

\subsection{Charakterystka zbioru danych}
\label{subsec:charakterystka}

W~projekcie wykorzystane zostaną dane ze zbioru \emph{FIFA 19 Complete Player Dataset}~\cite{fifa-dataset} z~serwisu \url{kaggle.com}. Każdy zawodnik posiada ponad 70 użytecznych atrybutów opisujących jego umiejętności gry w~piłkę nożną. Wśród nich znajdują się atrybuty opisujące m.in. kontrolę piłki, przyspieszenie czy celność strzałów z~dystansu. 

Większość atrybutów jest opisana za pomocą wartości całkowitych z~zakresu $[0, 99]$. Niektóre atrybuty zostały opisane liczbą całkowitą z~zakresu $[0,5]$. W~zestawie danych, pojawiły się atrybuty ciągłe opisujące wartość zawodnika, jego tygodniówkę, wzrost, wiek i~wagę oraz atrybuty kategoryczne opisujące preferowaną nogę (lewa lub prawa), typ budowy ciała oraz jego główną pozycję.

Każdemu zawodnikowi przypisany jest atrybut kategoryczny opisujący główną pozycję na boisku oraz 26 wskaźników numerycznych, określających jak dobry jest dany zawodnik na poszczególnych pozycjach.

Dodatkowo, zbiór danych został uzupełniony o~metadane opisujące samego zawodnika, takie jak numer na koszulce, nazwisko, link do zdjęcia, drużyna, narodowość czy międzynarodową reputację. 

\begin{figure}[t]
    \centering    
    \begin{tabular}{cc}
    \subfloat[\emph{Short Passing}]{
        \begin{tikzpicture}[thick, scale=0.4]
            \Huge
            \begin{axis}[ymin=0, area style]
                \addplot+[ybar interval,mark=no] plot coordinates { (0, 0) (5, 3) (10, 53) (15, 242) (20, 485) (25, 632) (30, 465) (35, 397) (40, 503) (45, 766) (50, 1350) (55, 2408) (60, 3553) (65, 3496) (70, 2309) (75, 1084) (80, 336) (85, 65) (90, 12) (95, 0) };
            \end{axis}
        \end{tikzpicture}
    } &
    \subfloat[\emph{Strength}]{
        \begin{tikzpicture}[thick, scale=0.4]
            \Huge
            \begin{axis}[ymin=0, area style]
                \addplot+[ybar interval,mark=no] plot coordinates { (0, 0) (5, 0) (10, 0) (15, 1) (20, 2) (25, 40) (30, 301) (35, 354) (40, 618) (45, 800) (50, 1121) (55, 2015) (60, 2631) (65, 3093) (70, 2810) (75, 2354) (80, 1182) (85, 539) (90, 294) (95, 4) };
            \end{axis}
        \end{tikzpicture}    
    } \\
    \subfloat[\emph{Agility}]{
        \begin{tikzpicture}[thick, scale=0.4]
            \Huge
            \begin{axis}[ymin=0, area style]
                \addplot+[ybar interval,mark=no] plot coordinates { (0, 0) (5, 0) (10, 1) (15, 11) (20, 108) (25, 201) (30, 682) (35, 667) (40, 639) (45, 805) (50, 1156) (55, 1813) (60, 2308) (65, 2770) (70, 2734) (75, 2216) (80, 1116) (85, 607) (90, 320) (95, 5) };
            \end{axis}
        \end{tikzpicture}
    } &
    \subfloat[\emph{GK Positioning}]{
        \begin{tikzpicture}[thick, scale=0.4]
            \Huge
            \begin{axis}[ymin=0, area style, ytick={0, 2000, 5000, 8000}]
                \addplot+[ybar interval, mark=no] plot coordinates { (0, 81) (5, 6337) (10, 7876) (15, 1831) (20, 4) (25, 1) (30, 3) (35, 3) (40, 31) (45, 88) (50, 227) (55, 326) (60, 460) (65, 450) (70, 251) (75, 127) (80, 51) (85, 11) (90, 1) (95, 0) };
            \end{axis}
        \end{tikzpicture}
    } \\

    \end{tabular}
    \caption{Przykładowe rozkłady wartości atrybutów z~podanego zbioru danych}
    \label{fig:rozklady-atrybutow}
\end{figure}

Na rysunku~\ref{fig:rozklady-atrybutow} zostały zaprezentowane przykładowe rozkłady wartości badanych atrybutów. W~głównej mierze, są to rozkłady normalne lub gruboogonowe. Ciekawy rozkład mają atrybuty opisujące umiejętności bramkarskie. Z~racji nietypowej specyfiki tej pozycji, atrybuty te mają rozkład dwumodalny. Wysoki lewy pik rozkładu opisuje zawodników z~pola, których umiejętności bramkarskie są niewielkie ale jest ich zdecydowanie więcej. Prawy niewielki pik opisuje rzeczywistych bramkarzy, którzy mają niewątpliwie większe umiejętności, jednak jest ich wyraźnie mniej. Obecność bramkarzy w~zestawie danych jest również jednym z~powodów dla którego część rozkładów ma gruby ogon po lewej stronie. 

\subsection{Przygotowanie atrybutów}
\label{subsec:atrybuty}

W~procesie przygotowania danych, należy zastanowić się które atrybuty należy pozostawić a które powinny zostać usunięte. Oprócz tego, wykorzystując wiedzę ekspercką na temat piłki nożnej można zastanowić się nad generacją nowych atrybutów. Kolejnym krokiem będzie transformacja danych do~postaci użytecznej do przeprowadzenia wnioskowania, poprzez uzupełnienie brakujących danych, skalowanie, normalizację i~zamianę atrybutów kategorycznych na numeryczne. Ostatecznie, należy określić które z~atrybutów będą traktowane jako obserwowalne, a~które jako ukryte.

Do zbioru danych dodane zostaną dodatkowe atrybuty wygenerowane na podstawie już istniejących oraz wiedzy eksperckiej. Takimi atrybutami mogą być wygenerowane cechy fizyczne przykładowo stosunek wagi do wzrostu lub stosunek przyspieszenia do prędkości maksymalnej.

Ze zbioru definitywnie należy usunąć atrybuty opisowe określające nieinteresujące nas cechy takie jak narodowość, aktualna drużyna, reputacja czy nazwisko. Dodatkowo, ze zbioru danych usunięte zostaną atrybuty opisujące umiejętność gry danego zawodnika na danej pozycji, ponieważ taka informacja znacząco upraszcza zadanie i~mija się z~celem projektu. W~przypadku pozostałych atrybutów należy przeprowadzić analizę korelacyjną, tak aby moć przeprowadzić preselekcję, tj. stwierdzić które atrybuty nie wnoszą dodatkowej informacji. Ostatecznie, aby móc grupować zawodników grających na podobnych pozycjach lecz o~różnych poziomiach umiejętności, wartości atrybutów zostaną przeskalowane względem ich średniej wartości.

Jako atrybut ukryty, potraktowana zostania optymalna pozycja na boisku. W~celu przeprowadzenia analizy poprawności badanego algorytmu grupowania, zakładamy że pozycja na boisku zapisana w~zbiorze danych jest jego rzeczywistą optymalną pozycją. Atrybut ten jest niezwykle ciekawy do analizy, ponieważ można go analizować na dwóch poziomach:

\begin{itemize}
    \item na poziomie ogólnym:
        \begin{itemize}
            \item bramkarz,
            \item obrońca,
            \item pomocnik,
            \item napastnik,
        \end{itemize}
    \item na poziomie szczegółowym:
    \begin{itemize}
        \item bramkarz,
        \item środkowy obrońca, 
        \item lewy/prawy boczny obrońca
        \item lewy/prawy wahadłowy,
        \item pomocnik ofensywny,
        \item lewy/środkowy/prawy pomocnik,
        \item pomocnik defensywny,
        \item napastnik,
        \item lewy/prawy skrzydłowy.
    \end{itemize}
\end{itemize}

Oprócz tego, w~zależności od wyników grupowania, można spróbować zdefiniować bardziej wyrafinowane pozycje m.in.: 
\emph{mezzala}, \emph{mediano}, \emph{trequartista} czy libero.

\section{Badane algorytmy}
\label{sec:algorytmy}

\subsection{Algorytm k-średnich}
\label{subsec:kmeans}
Celem algorytmu $k$-średnich jest pogrupowanie danych na $k$ grup w~taki sposób, aby zminimalizować sumę kwadratów odległości punktów od wyznaczonych centroidów (środków) grup. W~języku R, algorytm ten można wywołać za pomocą polecenia \texttt{kmeans}~\cite{kmeans}. 

W~pierwszym kroku algorytmu należy dobrać liczbę poszukiwanych grup oraz przyporządkować w~sposób losowy wszystkie obserwacje do grup. Następnie, w~$i$-tym kroku należy obliczyć położenie centroidu dla każdej grupy i~dla każdego punktu zaktualizować przynależności do grupy. Algorytm kończy się gdy w~danym kroku nie została zmieniona żadna przynależność lub przekroczono maksymalną liczbę kroków.

Domyślnie polecenie \texttt{kmeans} do znalezienia centroidów korzysta z~metody Hartigana-Wonga, jednak możliwa jest zamiana leżącego u~podstaw algorytmu na algorytm MacQueena lub Lloyda-Forgyego.

Głównym parametrem algorytmu $k$-średnich jest liczba szukanych grup $k$. Liczbę tą podajemy $a priori$, przed uruchomieniem algorytmu. Jednym ze sposobów na odpowiedni dobór parametru $k$ jest przeszukiwanie zupełne w~rozsądnie dobranym przedziale i~znalezienie takiej wartości, przy której wykres całkowitej sumy kwadratów odległości w~zbiorze danych ulega wypłaszczeniu.

Największym problemem algorytmu $k$-średnich jest dobór początkowego podziału na grupy. Możliwymi sposobami są losowy wybór $k$ 
przykładów ze zbioru danych lub ręczne podanie zbioru centroidów początkowych.

\subsection{Algorytmy grupowania hierarchicznego}
\label{subsec:hierarchical}

Grupowanie hierarchiczne w eksploracji danych i statystyce jest metodą analizy, która ma na celu zbudowanie hierarchii klastrów. W przeciwieństwie do wielu algorytmów dzielących zbiory danych na klastry w tym wypadku nie jest konieczne wstępne określenie liczby tworzonych klastrów. Alogorytmy grupowania  hierarchicznego dzielimy na dwa typy: aglomeracyjne (łączące) i deglomeracyjne (dzielące) \cite{maimonrokach}. Wynikiem użycia metod hierarchicznego grupowania jest zestaw zagnieżdżonych klastrów, które są zwykle prezentowane w dendrogramie. Dendrogram jest to wielopoziomowa hierarchia, w której klastry z jednego poziomu są połączone i tworzą większe klastry na kolejnych poziomach.

Przy grupowaniu hierarchicznym wykorzystywane są różne sposoby na określanie połączenia między dwoma klastrami. Są to miary odległości pomiędzy dwoma klastrami. Do najbardziej popularnych, możliwych do wyboru w algorytmach wbudowanych w język R należą\cite{rodriguez2015}:

\begin{itemize}
    \item \textbf{pojedyncze połączenie} - minimalna odległość między obserwacją w jednym klastrze a obserwacją w innym klastrze,
    \item \textbf{kompletne połączenie} - maksymalna odległość między obserwacją w jednym klastrze a obserwacją w innym klastrze,
    \item \textbf{średnie połączenie} - średnia odległość między obserwacją w jednym klastrze a obserwacją w innym klastrze,
    \item \textbf{połączenie centroidalne} - odległość pomiędzy centroidami klastrów,
    \item \textbf{połączenie medianowe} - mediana odległości między obserwacją w jednym klastrze a obserwacją w innym klastrze
    \item \textbf{połączenie Ward’a} - suma kwadratów odchyleń od punktów do centroidów
    \item \textbf{połączenie McQuitty’ego} - gdy dwa klastry A i B zostaną połączone, odległość do nowego klastra C jest średnią odległości A i B do C
\end{itemize}


Należy pamiętać o tym, że w zależności od zbioru danych, niektóre metody mogą działać lepiej od innych.

\subsubsection{Algorytmy aglomeracyjne}

Metody aglomeracyjne są najbardziej popularną metodą grupowania hierarchicznego. Zaraz po rozpoczęciu algorytmu każdy obiekt ze zbioru danych jest traktowany jako pojedynczy klaster. Kolejno klastry są  łączone, do momentu gdy wszystkie klastry zostaną scalone w jeden duży klaster zawierający wszystkie obiekty zbioru danych. W języku R dokonujemy grupowania algorytmem aglomeracyjnym przy użyciu polecenia \texttt{hclust}.

\subsubsection{Algorytmy deglomeracyjne}
Metody deglomeracyjne są przeciwieństwem metod aglomeracyjnych. Na początku działania algorytmu wszystkie obserwacje znajdują się w jednym klastrze. W kolejnych iteracjach klastry są dzielone na mniejsze. Proces dzielenia klastrów powtarzany jest do momentu, gdy każda obserwacja znajduje się we właściwym klastrze tj. do momentu gdy liczba klastrów będzie równa liczbie obserwacji. W języku R dokonujemy grupowania deglomeracyjnego przy użyciu polecenia \texttt{diana}.

\subsection{Algorytmy grupowania rozmytego}
\label{subsec:fuzzy}
Algorytmy grupowania rozmytego bazują na założeniu że pojedyncza obserwacja może równocześnie należeć do wielu grup. To jak bardzo przykład należy do danej grupy $v$, opisuje \emph{funkcja przynależności} $\mu_{v}(x)$, która może przyjmować wartości z~przedziału $[0,1]$. Dla pojedynczej obserwacji $x$ spełnione są następujące własności:

\begin{equation}
    \sum_{v=1}^{k} \mu_{v}(x)    
\end{equation}

\begin{equation}
    \forall{v} \quad \mu_{v}(x) \geq 0     
\end{equation}

Podejście rozmyte może potencjalnie dać więcej korzyści, biorąc pod uwagę temat projektu. W~nowoczesnym futbolu, co raz częściej możemy spotkać się z~sytuacją w~której zawodnicy muszą umieć grać na kilku (jak nie wszystkich) pozycjach równocześnie.

W~projekcie porównamy działanie dwóch algorytmów rozmytego grupowania: fuzzy C-means oraz FANNY.

Podstawowym algorytmem grupowania rozmytego jest rozszerzenie algorytmu $k$-średnich na zbiory rozmyte, znany w~literaturze jako rozmyte $k$-średnich lub fuzzy C-means~\cite{cmeans}.

Algorytm ten nie różni się wiele od wersji oryginalnej. Jedyną różnicą jest zmiana sposobu określania przynależności do grupy, zamiast szukania najbliższej grupy, obliczamy przynależność do wszystkich grup równocześnie. W~pierwszym kroku algorytmu należy dobrać liczbę poszukiwanych grup oraz przyporządkować w~sposób losowy początkowe wartości funkcji przynależności do wszystkich grup. Następnie, w~$i$-tym kroku należy obliczyć położenie centroidu dla każdej grupy, a~dalej dla każdego punktu zaktualizować funkcje przynależności do wszytkich grup. Algorytm kończy działanie w~momencie gdy wartości funkcji przynależności zmieniły się o~wartość mniejszą niż $\epsilon$ lub osiągnięta została maksymalna liczba kroków.

Algorytm rozmytych $k$-średnich został zaimplementowany w~pakiecie \texttt{e1017} jako funkcja \texttt{cmeans}. Parametrami wymagającymi strojenia jest liczba grup, funkcja odległości, maksymalna liczba iteracji oraz stopień rozmycia~\cite{fanny}.

Drugim badanym przez nas algorytmem będzie algorymt FANNY opracowany przez Kaufmana i~Rousseeuwa~\cite{kaufman2009finding} a~rozszerzony przez Maechlera. Algorytm ten minimalizuje następującą funkcję celu:

\begin{equation}
    \sum_{v=1}^{k} \frac{\sum_{i=1}^{n}\sum_{j=1}^{n} \mu^{r}_{v}(x_{i}) \mu^{r}_{v}(x_{j}) d(x_{i}, x_{j})}{2 \sum_{j=1}^{n} \mu^{r}_{v}(x_{j})} 
\end{equation}

gdzie~$n$ oznacza liczbę obserwacji w~zbiorze danych, $k$~jest liczbą grup, $r$~jest \emph{wykładnikiem przynależności}, natomiast~$d(x_{i}, x_{j})$ jest wybraną funkcją niepodobieństwa przykładów~$x_{i}$~i~$x_{j}$.


Algorytm ten został zaimplementowany w~pakiecie \texttt{cluster} jako funkcja \texttt{fanny}. Parametrami tej funkcji są: liczba grup, wykładnik przynależności, funkcja odległości oraz maksymalna liczba iteracji.

\section{Miary}
\label{sec:miary}

\subsection{Miary niepodobieństwa}
\label{subsec:podobienstwo}
Wszystkie opisane wcześniej algorytmy automatycznego grupowania bazują na pewnej funkcji $d(x_{i}, x_{j})$ zwanej miarą niepodobieństwa lub odległości. Opisuje ona jak \emph{różne} są do siebie dwa przykłady. Przykładowymi miarami niepodobieństwa/odległości są: odległość euklidesowa, odległość Minkowskiego oraz odległosć Manhattan.

Najbardziej podstawową miarą niepodobieństwa jest odległość euklidesowa czyli nic innego jak pierwiastek z~sumy kwadratów różnic pomiędzy poszczególnymi atrybutami. Odległość ta dana jest wzorem:

\begin{equation}
    d_{e}(x, y) = \sqrt{\sum_{k=1}^{n} (x_k - y_k)^{2}}
\end{equation}

Bardziej ogólną wersją odleglości euklidesowej jest odległość Minkowskiego~\cite{irani2016clustering}. Jest ona dana wzorem:

\begin{equation}
    d_{mink}(x, y) = \sqrt[p]{\sum_{k=1}^{n} (x_k - y_k)^{p}}
\end{equation}

Dla $p=2$ otrzymujemy odległość euklidesową, natomiast dla $p \xrightarrow{} \infty$ otrzymujemy odległość Czebyszewa, która jest zdefiniowana jako największa różnicą pomiędzy parą atrybutów:

\begin{equation}
    d_{cheb}(x, y) = \max_{k} |x_k - y_k|
\end{equation}

Ostatnią rozważaną miarą podobieństwa jest odległość Manhattan. Bierze ona swoją nazwę od regularnej siatki ulic w~dzielnicy Manhattan w~Nowym Yorku. Odległość dwóch punktów w tej metryce to suma wartości bezwzględnych różnic ich współrzędnych.

\begin{equation}
\label{eqn:manhattan}
d_{manh}(x, y) = \sum_{k=1}^{n}|x_k - y_k|
\end{equation}

\subsection{Miary jakości grupowania}
\label{subsec:jakosc}

\subsubsection{Podejście nadzorowane}
Automatyczne grupowanie jest dziedziną uczenia nienadzorowanego, co sprawia że określenie jakości działania algorytmu jest bardziej złożone niż w~przypadku uczenia nadzorowanego. Częśc miar wykorzystuje informację o~atrybucie ukrytym, jednak nie w~takim stopniu jak typowe miary klasyfikacji jak na przykład macierz pomyłek (i~wszystkie powiązanie z~nią wskaźniki). Wskaźniki te to skorygowany index Rand, wskaźnik skorygowanej informacji wzajemnej, wskaźnik jednorodności, wskaźnik kompletności, wskaźnik Fowlkesa-Mallowesa oraz V-wskaźnik~\cite{scikit}. 

Wskaźniki te mogą dać informację jak poprawne jest nasze grupowanie, jednak do ich obliczenia potrzebna będzie informacja o~wartości atrybutu ukrytego. Z~racji mnogości omawianych pozycji oraz ich względnej nieseparowalności, tego typu rozważania należy prowadzić na poziomie ogólnym, opisanym w~sekcji~\ref{subsec:atrybuty}.

\subsubsection{Podejście nienadzorowane}
Drugim, znacznie ciekawszym, podejściem do problemu jest pominięcie wartości atrybutu ukrytego i~skupienie się na jakości wewnętrznej samego grupowania. Wartość danej miary jakości jest określana na podstawie samego podziału, bez dodatkowej informacji o~wartości atrybutu ukrytego. Miary tego typu skupiają się na głównie na separowalności samych grup oraz na tym jak są dobrze wewnętrznie spójne i~określone. Metryki tego typu to wariancja wewnątrzgrupowa, wskaźnik sylwetki, wskaźnik Calinskiego-Harabasza oraz indeks Daviesa-Bouldina.

Na podstawie miar tego typu można spróbować dobrać najlepszą ilość grup na jaką można podzielić zbiór danych. Im mniej grup tym więcej informacji tracimy, natomiast za duża liczba grup spowoduje nadmierne dopasowanie. Wykorzystując przykładowo wariancję wewnątrzgrupową można spróbować dobrać parametr $k$ w~algorytmie $k$-średnich, stosując regułę \emph{łokcia}. Na wykresie rysuje się zależność sumy wariancji wewnątrzgrupowej od liczby grup. Punkt przełamania wykresu (nazywane łokciem) wskaże najlepszą liczbę grup, która obejmuje dobrze generalizuje oraz unika nadmiernego dopasowania.

\section{Otwarte kwestie}
\label{sec:otwarte-kwestie}
Pierwszym spostrzeżeniem po przyjrzeniu się danym było zauważenie że klasyczne algorytmy grupowania mogą mieć problem z~połączeniem ze sobą dwóch zawodników, którzy w~rzeczywistości grają na tej samej pozycji lecz na zupełnie innym poziomie. Przykładowo, Manuel Neuer oraz Pavels Steinbors obaj grają na pozycji bramkarza lecz jeden z~nich gra w~drużynie mistrza Niemiec a drugi w~pierwszoligowej Arce Gdynia -- bezwzględne wartości ich statystyk będą odmiennie różne. Początkowo zaproponowaliśmy zamianę pierwotnych wartości atrybutów na ich róznicę od średniej, jednak rozwiązanie to wydaje się niepewne. W~przypadku niezadowalających wyników trzeba będzie zastosować inną metodę normalizacji danych. Ciekawym podejściem do tego problemu będzie wykorzystanie korelacyjnych miar odległości pomiędzy dwoma przykładami lub wykorzystujące odległośc kosinusową. Metody obliczające niepodobieństwo w~ten sposób zostały zaimplementowane w~pakiecie \texttt{proxy} języka~R~\cite{proxy}.

Dodatkową sprawą wymagającą działania jest sama liczba atrybutów. W~opisywanym zbiorze danych liczba atrybutów sięga prawie stu, co przy braku przeprowadzenia selekcji, może znacząco osłabić działanie algorytmów grupowania. W~sekcji~\ref{subsec:atrybuty}~opisane zostały metody usuwania atrybutów ze zbioru takie jak proste usuwanie niepotrzebnych kolumn lub analiza korelacyjna. W~przypadku gdy niezadowalających wyników, ręczną analizę korelacyjną można zastąpić automatyczną analizą głównych składowych (\emph{ang. Principal Component Analysis}, która pozwala na zmniejszenie wymiarowości problemu, kosztem części informacji zawartej w~zbiorze. 

\bibliographystyle{abbrv}
\bibliography{bibliography}

\end{document}